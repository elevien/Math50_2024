\include{./../latex/notes_style.tex}
%--------------------------------------------------------------------------------------------------------------------------------

\begin{document}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%  EXERCISES  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\title{Exercise set 2}

\maketitle


\section{Exercises}

 % ------------------------------------------------------------------------------------------------------------------------------------------
\begin{exercise}[Computing conditional averages]
Consider a random variable $Y = (Y_1,Y_2)$ which takes values in the sample space
\begin{equation*}
S = {\mathbb N} \times {\mathbb N} = \{(i,j):i,j \in {\mathbb N}\}
\end{equation*}
That is, the sample space consists of all possible pairs of numbers $(i,j)$. Now suppose we have some data:
\begin{equation*}
\{(1,2),(1,2),(3,1),(1,4),(3,3),(2,2),(1,5)\}
\end{equation*}
Give you best estimates of the following (either by hand, with Python, or a calculator)
\begin{enumerate}[label=(\alph*)]
\item $E[Y_1]$
\item $E[Y_1|Y_2=2]$
\item $E[Y_2|Y_1=1]$
\item $E[Y_2|Y_1>1]$
\end{enumerate}




 % ------------------------------------------------------------------------------------------------------------------------------------------
\begin{exercise}[Independence and conditional expectation]
Let $X$ and $Y$ be two random variables with (discrete) sample spaces $S_X$ and $S_Y$.  
\begin{enumerate}[label=(\alph*)]
\item Prove that if $X$ and $Y$ are independent $E[X|Y=y]=E[X]$ and $E[Y|X=x]=E[Y]$ for all $x \in S_X$ and $y \in S_Y$.  You may assume $S_X$ and $S_Y$ are countable (i.e. discrete) spaces. 
\item Prove the tower property of expectation that is stated in the class notes. 
\item Show that if $X$ and $Y$ are independent, then 
\begin{equation*}
{\rm var}(X+Y) = {\rm var}(X) + {\rm var}(Y)
\end{equation*} 
Hint: use the formula
\begin{equation*}
{\rm var}(X) = E[X^2]-E[X]^2
\end{equation*}
%\item We can define condition variance as
%\begin{equation*}
%{\rm var}(X|Y=y) = \sum_{x\in S_X}(x-E[X|Y])^2P(X=x|Y=y)
%\end{equation*}
\end{enumerate}
\end{exercise}




\end{exercise}
 % ------------------------------------------------------------------------------------------------------------------------------------------
\begin{exercise}[Aspects of the binomial distribution] Suppose $Y_1$ and $Y_2$ are two independent binomial distributions:
\begin{align*}
Y_1 &\sim {\rm Binomial}(N_1,p_1)\\
Y_2 &\sim {\rm Binomial}(N_2,p_2)
\end{align*}
with $p_1,p_2 \in (0,1)$ and $N_1,N_2 \in {\mathbb N}$.
\begin{enumerate}[label=(\alph*)]
\item If $p = p_1 = p_2$ then what is the distribution of $Y_1 + Y_2$? Explain your reasoning.
\item Confirm you answer to part (a) with simulations with $N_1 = 100$, $N_2 = 10$ and $p=0.3$. 
\item Now suppose $p_1 \ne p_2$. Let 
\begin{equation*}
Y_3 \sim {\rm Binomial}\left(N_1+N_2,\frac{N_1}{N_1+N_2}p_1 + \frac{N_2}{N_1+N_2}p_2\right)
\end{equation*}
Here is an {\bf erroneous} argument for why $Y_3$ might have the same distribution as $Y_1 + Y_2$ (it doesn't!): 

\vspace{0.2cm}
\begin{quote}\textcolor{black}{$Y_1+Y_2$ is the sum of $N$ Bernoulli random variables. Denote these as $X_1,X_2,\dots,X_N$ where $N = N_1 +N_2$.  Assume they are in order, so that the first $N_1$ terms are the Bernoulli random variables corresponding to the first binomial distribution (the one with success probability $p_1$). Note that with this notation we are not specifying whether $X_i$ comes from the first or second Bernoulli sequence.  If we randomly select one of these, $X_i$, then the chance it is equal to $1$ is 
\begin{equation*}
P(X_i = 1) = P(X_i = 1|i\le N_1)P(i\le N_1) + P(X_i = 1|i> N_1)(i> N_1).
\end{equation*} 
Now observe that 
\begin{align*}
 P(i\le N_1) &= N_1/(N_1+N_2)\\
 P(i> N_1) &= N_2/(N_1+N_2)\\
 P(U_i = 1|i\le N_1) &= p_1\\
 P(U_i = 1|i> N_1) &= p_2.
 \end{align*}
 Plugging these into the formula for $P(X_i = 1)$ gives the probability of success in the definition of $Y_3$. 
 }\end{quote}
 \item Explain why this argument above is flawed.  Hint: Is the variable $X_i$ and $X_j$ independent for all $i\ne j$? If not, why is independence important? 
\item Confirm that the argument above is incorrect using simulations; that is, confirm via simulations of an example that $Y_3$ does not have the same probability distribution as $Y_1 + Y_2$. You can do this many ways, for example, by plotting $P(Y_3>k)$ as a function of $k$ and comparing to $P(Y_1+Y_2>k)$. 
\end{enumerate}



\end{exercise}


 % ------------------------------------------------------------------------------------------------------------------------------------------
\begin{exercise}[Election modeling]\label{ex:election}
Suppose again that we are interested in predicting the outcome of an election with two candidates and $N$ voters. Based on or polling data, people's preferences are equally split between the two candidates ($q=1/2$). However, there is one particular person -- person 1 -- who is particularly influential. If person 1 votes for candidate one, then everyone else votes for candidate one, while if person 1 votes for candidate 2, everyone sticks with their original preference. 

The vote total for candidate $1$ can be written as 
\begin{equation*}
Y = \sum_{i=1}^N y_iy_1
\end{equation*}
where 
\begin{equation*}
Y_i \sim {\rm Bernoulli}(1/2),\quad i=1,\dots,N
\end{equation*}
Each $Y_i$ is $0$ if they vote for candidate $1$ and $1$ if they vote for candidate $2$. 
$Y_1$ represents the vote of the very influential person 
The code below simulates this model.


\begin{Verbatim}
def sampleY(N,n_samples):
  y = np.zeros(n_samples)
  for i in range(n_samples):
    ys = np.random.choice([0,1],N)
    y[i] = np.sum(ys)*ys[0]
  return y
\end{Verbatim}

\begin{enumerate}[label=(\alph*)]
\item Let $\phi$ denote the fraction of votes for candidates $1$. How do you think the CV (coefficient of variation) of $\phi$ depends on $N$ as $N$ becomes large? Test you hypothesis with simulations. 
\item What are $E[\phi]$ and $E[\phi|Y_1 = 0]$. Confirm your answers with simulations of the model. 
\end{enumerate}
\end{exercise}


 % ------------------------------------------------------------------------------------------------------------------------------------------
%\begin{exercise}[Normality]
%Do you expect the following variables to be Normal or not. Explain your answer
%\begin{enumerate}[label=(\alph*)]
%\item The height of pine trees in new Hampshire. 
%\item The age (in days) of used cars for sale
%\item The finishing time of racers in the Boston marathon. 
%\item The finishing time of racers in the Boston marathon. 
%\end{enumerate}
%
%\end{exercise}


\end{document}


 % ------------------------------------------------------------------------------------------------------------------------------------------
\begin{exercise}[Conditioning with continuous variables] Let
\begin{align*}
Z_1 &\sim {\rm Normal}(0,1)\\
Z_1 &\sim {\rm Normal}(1,2)
\end{align*} 
Compute each of the following using Python
\begin{enumerate}[label=(\alph*)]
\item $P(Z_1 + Z_2>3)$
\item $P(Z_1 + Z_2>3|Z_1<-1)$
\item $P(Z_2Z_2>0|Z_1+Z_2<4)$
\item Suppose we have a model of hemoglobin levels for men as 
\begin{equation*}
Z \sim {\rm Normal}(15.8,1.4)
\end{equation*}
(these numbers are in the ballpark but I kinda guessed, so don't try to diagnoise your anemia based on this problem). 
Some has Polycythemia if $Z>17.1$. Given that someone has does not have Polycythemia, what is the chance that they are anemic
\end{enumerate}
\end{exercise}



 % ------------------------------------------------------------------------------------------------------------------------------------------
\begin{exercise}[Testing the central limit Theorem]
Suppose 
\begin{equation*}
U_i \sim {\rm Uniform}(-L,L),\quad i=1,\dots,N
\end{equation*}
and let 
\begin{equation*}
S_N = \sum_{i=1}^N U_i
\end{equation*}
\begin{enumerate}[label=(\alph*)]
\item Using simulations, confirm that\footnote{If you know calculus you should be able to derive this, but you don't have to. }
\begin{equation*}
{\rm var}(U_i) = \frac{L^2}{3}.
\end{equation*}
In particular, make a plot of ${\rm var}(U_i)$ as a function of $L$. 
\item What does the CLT tell us about how ${\rm var}(S_N)$ depends on $N$ for large $N$? 
\item Confirm your answer to part $(b)$ with simulations. 
\end{enumerate}

\end{exercise}



\begin{exercise}
The {\bf random walk} is a foundational model in nearly every area of science, from theoretical physics to economics. It describes the "motion" of a variable whose behavior is governed by randomness motion over time. It is defined as follows. 

Let $X_0=0$ and define $X_k$ for $k=1,2,3,\dots$ by the recursive formula 
\begin{align}\label{eq:rw}
X_{k+1} = X_{k}  + \Delta(2U_k - 1)
\end{align}
where $\Delta$ is a constant and 
\begin{equation*}
U_k  \sim {\rm Bernoulli}(1/2)
\end{equation*}
are iid random variables. 


We can think of $X_k$ as the position of a person who is randomly walking with 50-50 chance of the moving to the left or right by $\Delta$ at each time-step. The entire sequence $X_0,X_1,X_2,\dots$ is referred to as the path of the random walker. 
\begin{enumerate}[label=(\alph*)]
\item Write a python function simulaterw(Delta,K) which simulates a random walk for $N$ steps. You code should return the entire path in a numpy array. Make some plots of $X_k$ vs. $k$. 
\item What is $E[X_k|X_{k-1}=2]$ and $E[X_k]$? 
%\item A much harder problem is to calculate $E[X_k|X_{k-1}>0]$. Don't calculate it, but explain why this is more difficult to calculate then the quantities above. 
\item Using the central limit theorem, derive an approximation of the {\bf mean squared displacement}
\begin{equation*}
{\rm MSD}(X_k) = E[X_k^2]
\end{equation*}
(this is just another name for the variance that is used in the context of random walks)
Verify your approximation by plotting ${\rm MSD}(X_k)$ as a function of $N$. 
\end{enumerate}

%You can start by modifying the following function:

\end{exercise}




 % ------------------------------------------------------------------------------------------------------------------------------------------
\begin{exercise}[Test scores]
Her we revisit the test score data from example 3 in class, but now we will have a model for the test scores. In particular, let $Y$ be the score and $X$ be the mother's high school education. We will assume that  
\begin{equation*}
Y = a X + b + \epsilon
\end{equation*}
where 
\begin{equation*}
X \sim {\rm Bernoulli}(q)
\end{equation*}
and 
\begin{equation*}
\epsilon \sim {\rm Normal}(0,\sigma_{\epsilon})
\end{equation*}
is independent of $X$. 
Another way to say this is that 
\begin{equation*}
Y|(X=x) \sim {\rm Normal}(aX+b,\sigma_{\epsilon});
\end{equation*}
that is, the condition distribution of $Y$ given the mother's equation is Normal. 

\begin{enumerate}[label=(\alph*)]
\item Using mathematic (as opposed to simulations) compute $E[Y]$ (the marginal average of the scales) and $E[Y|X=0]$ (the average scores given that mothers did not attend high school). You are computing these within the context of the model above, not the data. 
\item Find a formula for ${\rm var}(Y)$.  Note that this is the marginal variance. Hint: use that $\epsilon$ and $X$ are independent. What is the variance of the sum of two independent variables? 
\item How would you estimate $a$ and $b$ from the data?  Test your procedure on simulated data by estimating $a$ and $b$. 
%\item   What do the parameters $a$ and $b$ represent in terms of conditional averages of scores? That is, express $a$ and $b$ in terms of averages conditioned on values of $Y$. 
% \item Based on the data can you find some ballpark estimates of all the parameters in the model? Hint: to estimate $\sigma_{\epsilon}$ you will want to look at the standard deviation of scores conditioned on mother's high school education. There are different ways to do this, but for the purpose of \underline{ballpark} estimates it doesn't matter which way you choose. 
%\item Within the model (as opposed to in real life), what is the chance that a student whose mother attended high school gets a score greater than $75$? Your answer should use the parameters values you computed in part 3. 
%You can answer this question either with simulations, or using the bell-curve (``$z$-scores'').
\end{enumerate}
\end{exercise}
\end{document}




 % ------------------------------------------------------------------------------------------------------------------------------------------
\begin{exercise}[More on expectation]
Prove each of the following statements and verify with simulations. You can use a probability model from a previous exercise or make up a new one, but you must explain your model and why it satisfied the assumptions of each identity. 
\begin{enumerate}[label=(\alpha*)]
\item Suppose that $X$ and $Y$ are two random variables on (discrete) sample spaces $S_X$ and $S_Y$. 
\begin{equation}
E[X^2]>E[X]^2
\end{equation}
\end{enumerate} 
\end{exercise}
 % ------------------------------------------------------------------------------------------------------------------------------------------
%\begin{exercise}[The CLT theorem vs. large deviations]
%Suppose 
%\begin{equation*}
%S_N \sim {\rm Binomial}(N,p)
%\end{equation*}
%with $N = 100$ and $p=0.1$. 
%Let 
%\begin{equation*}
%F_{n} = P(S_N>n)
%\end{equation*} 


%\end{exercise}



%More formally, how can we understand this in terms of Baye's Theorem? 
%
%That is, what if we FIX the value of $X$. 
%\begin{equation}
%f(y|X = x) = \frac{f(y,x)}{f(x)}
%\end{equation}
%$f(x)$ is the area under the curve 
%This means 

%
%\begin{exercise}
%\href{https://colab.research.google.com/drive/1PPFwE4GUzsr707s3mPhGRs7-TYlHxND2#scrollTo=3CXQuszHxsvn&line=19&uniqifier=1}{Conditioning in the regression model}
%\end{exercise}














\end{document}